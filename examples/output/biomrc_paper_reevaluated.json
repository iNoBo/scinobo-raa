{
 "publication_info": {
  "Identifiers": "MD5: 30AADB2C0CFD18CBA5E1A2CC84B4858E\narXiv: arXiv:2005.06376v1[cs.CL]",
  "Title": "BIOMRC: A Dataset for Biomedical Machine Reading Comprehension",
  "Authors": "Petros Stavropoulos\nDimitris Pappas\nIon Androutsopoulos\nRyan Mcdonald",
  "Abstract": "We introduce BIOMRC, a large-scale clozestyle biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset, and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.",
  "Sections": "Introduction\nDataset Construction\nCandidates\nExperiments and Results\nMethods\nResults on BIOMRC LITE\nResults on BIOMRC TINY\nPassage\nRelated work\nConclusions and Future Work\nFigure 2 :\nFigure 3 :\nTable 1 :\nTable 4 :\nTable 5 :\nAcknowledgments\nAcknowledgments",
  "Claim": "Skipped",
  "Claimer": "Skipped",
  "Topic": "Skipped",
  "FOS Pred 1": "Skipped",
  "FOS Pred 2": "Skipped",
  "FOS Pred 3": "Skipped"
 },
 "research_artifacts": [
  {
   "RA Cluster": "dataset_cluster_13",
   "Research Artifact": "PUBTATOR",
   "Type": "dataset",
   "Research Artifact Score": 0.9985764992744596,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.7275873052990344,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Wei et al., 2012) (100.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_cluster_12",
   "Research Artifact": "BIOMRC LARGE",
   "Type": "dataset",
   "Research Artifact Score": 0.9966714275219458,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 50.0,
   "Reused Score": 0.9374324468261144,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Table 2) (100.0%)",
   "Mentions Count": 4
  },
  {
   "RA Cluster": "dataset_cluster_11",
   "Research Artifact": "BIOMRC TINY",
   "Type": "dataset",
   "Research Artifact Score": 0.9974105567580166,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.8433439595160438,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Table 3) (100.0%)",
   "Mentions Count": 3
  },
  {
   "RA Cluster": "dataset_cluster_10",
   "Research Artifact": "CNN | Daily Mail",
   "Type": "dataset",
   "Research Artifact Score": 0.9937967143286052,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.6851874927003547,
   "Licenses": "N/A | N/A (100.0%)",
   "Versions": "N/A | N/A (100.0%)",
   "URLs": "",
   "Citations": "(Hermann et al., 2015) (100.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_cluster_9",
   "Research Artifact": "MLP",
   "Type": "dataset",
   "Research Artifact Score": 0.9418990588677205,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9439711920455572,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_cluster_8",
   "Research Artifact": "SCIBERT-SUM-READER",
   "Type": "dataset",
   "Research Artifact Score": 0.9939429264961267,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.80524092810768,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_cluster_3",
   "Research Artifact": "BIOMRC LITE",
   "Type": "dataset",
   "Research Artifact Score": 0.998244262671391,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 75.0,
   "Reused Score": 0.9056172188866887,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Table 3) (100.0%)",
   "Mentions Count": 4
  },
  {
   "RA Cluster": "dataset_cluster_6",
   "Research Artifact": "SUM-READER",
   "Type": "dataset",
   "Research Artifact Score": 0.9985825064607389,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 50.0,
   "Reused Score": 0.9301800328764687,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 2
  },
  {
   "RA Cluster": "dataset_cluster_5",
   "Research Artifact": "CLICR",
   "Type": "dataset",
   "Research Artifact Score": 0.9965863675222659,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.723796682694433,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 5
  },
  {
   "RA Cluster": "dataset_cluster_4",
   "Research Artifact": "UMLS",
   "Type": "dataset",
   "Research Artifact Score": 0.9815927641027737,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Soysal et al., 2017) (50.0%)\n(Lindberg et al., 1993) (50.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_cluster_1",
   "Research Artifact": "BIOASQ",
   "Type": "dataset",
   "Research Artifact Score": 0.9978267084511213,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0.5245999920700455,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Tsatsaronis et al., 2015) (100.0%)",
   "Mentions Count": 3
  },
  {
   "RA Cluster": "dataset_cluster_2",
   "Research Artifact": "Google's Natural Questions",
   "Type": "dataset",
   "Research Artifact Score": 0.9925318380292343,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Kwiatkowski et al., 2019) (100.0%)",
   "Mentions Count": 2
  },
  {
   "RA Cluster": "dataset_cluster_0",
   "Research Artifact": "Natural Questions",
   "Type": "dataset",
   "Research Artifact Score": 0.998683488598035,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Kwiatkowski et al., 2019) (100.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_unnamed",
   "Research Artifact": "Unnamed_3",
   "Type": "dataset",
   "Research Artifact Score": 0.9553466336502345,
   "Owned": "Yes",
   "Owned Percentage": 100.0,
   "Owned Score": 0.967448113840002,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOREAD",
   "Type": "dataset",
   "Research Artifact Score": 0.9952237406146595,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0.9951294330379342,
   "Reused": "Yes",
   "Reused Percentage": 33.33333333333333,
   "Reused Score": 0.7787569818115189,
   "Licenses": "",
   "Versions": "",
   "URLs": "https://www.ncbi.nlm.nih.gov/pmc/ (100.0%)",
   "Citations": "(2018) (100.0%)",
   "Mentions Count": 6
  },
  {
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "MRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9525899503696372,
   "Owned": "Yes",
   "Owned Percentage": 50.0,
   "Owned Score": 0.9965117699052424,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0,
   "Licenses": "publicly available (100.0%)",
   "Versions": "",
   "URLs": "",
   "Citations": "(2018) (50.0%)\n(Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017) (50.0%)",
   "Mentions Count": 2
  },
  {
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9948086510614906,
   "Owned": "Yes",
   "Owned Percentage": 4.3478260869565215,
   "Owned Score": 0.9308886318239523,
   "Reused": "Yes",
   "Reused Percentage": 17.391304347826086,
   "Reused Score": 0.8353106568397248,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(2018) (50.0%)\n(Hermann et al., 2015) (25.0%)\n(Pampari et al., 2018;Ben Abacha et al., 2019;Zhang et al., 2018) (25.0%)",
   "Mentions Count": 23
  },
  {
   "RA Cluster": "dataset_cluster_7_14",
   "Research Artifact": "PUBMED CENTRAL",
   "Type": "dataset",
   "Research Artifact Score": 0.9980421370969207,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9537291994702725,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "dataset_cluster_7_14",
   "Research Artifact": "PUBMED",
   "Type": "dataset",
   "Research Artifact Score": 0.9648395270085485,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 50.0,
   "Reused Score": 0.787620902895064,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Wei et al., 2012) (100.0%)",
   "Mentions Count": 2
  },
  {
   "RA Cluster": "software_cluster_16",
   "Research Artifact": "Attention Sum Reader",
   "Type": "software",
   "Research Artifact Score": 0.9995643310682689,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9860577199711463,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Kadlec et al., 2016) (33.33333333333333%)\n(Cui et al., 2017) (33.33333333333333%)\n(2018) (33.33333333333333%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_15",
   "Research Artifact": "MRC",
   "Type": "software",
   "Research Artifact Score": 0.9995238420628769,
   "Owned": "Yes",
   "Owned Percentage": 100.0,
   "Owned Score": 0.9969517176786158,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.8766359215456292,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Devlin et al., 2019) (100.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_7",
   "Research Artifact": "neural MRC",
   "Type": "software",
   "Research Artifact Score": 0.9991310550262936,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9823642384926757,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Kadlec et al., 2016) (25.0%)\n(Cui et al., 2017) (25.0%)\n(Devlin et al., 2019) (25.0%)\n(2018) (25.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_13",
   "Research Artifact": "N/A | N/A",
   "Type": "software",
   "Research Artifact Score": 0.9923905271043041,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.8729217545803231,
   "Licenses": "N/A | N/A (100.0%)",
   "Versions": "N/A | N/A (100.0%)",
   "URLs": "",
   "Citations": "(Kadlec et al., 2016) (33.33333333333333%)\n(Cui et al., 2017) (33.33333333333333%)\n(2018) (33.33333333333333%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_11",
   "Research Artifact": "BERT-based model",
   "Type": "software",
   "Research Artifact Score": 0.9979614751005492,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9569080256526209,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Beltagy et al., 2019) (50.0%)\n(Devlin et al., 2019) (50.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_10",
   "Research Artifact": "SCIBERT",
   "Type": "software",
   "Research Artifact Score": 0.996551151837178,
   "Owned": "Yes",
   "Owned Percentage": 20.0,
   "Owned Score": 0.9097216700698656,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9692495104557084,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Beltagy et al., 2019) (50.0%)\n(Devlin et al., 2019) (50.0%)",
   "Mentions Count": 5
  },
  {
   "RA Cluster": "software_cluster_9",
   "Research Artifact": "softmax",
   "Type": "software",
   "Research Artifact Score": 0.9549422460788782,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9744656856558018,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_8",
   "Research Artifact": "BIOMRC LARGE",
   "Type": "software",
   "Research Artifact Score": 0.9931787901325224,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9924830117122081,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_3",
   "Research Artifact": "SCIBERT-SUM-READER",
   "Type": "software",
   "Research Artifact Score": 0.9981545199275326,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 50.0,
   "Reused Score": 0.8986387736449327,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 2
  },
  {
   "RA Cluster": "software_cluster_6",
   "Research Artifact": "BIOMRC LITE",
   "Type": "software",
   "Research Artifact Score": 0.9946573972267864,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9878653921355747,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 2
  },
  {
   "RA Cluster": "software_cluster_5",
   "Research Artifact": "AS-READER | AOA-READER",
   "Type": "software",
   "Research Artifact Score": 0.9969794381370849,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 50.0,
   "Reused Score": 0.9302531548080855,
   "Licenses": "N/A | N/A (100.0%)",
   "Versions": "N/A | N/A (100.0%)",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 2
  },
  {
   "RA Cluster": "software_cluster_4",
   "Research Artifact": "AOA-READER",
   "Type": "software",
   "Research Artifact Score": 0.9335151552816192,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.8787890998437556,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Table 3) (100.0%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_1",
   "Research Artifact": "neural",
   "Type": "software",
   "Research Artifact Score": 0.9132799423787455,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.8905575462620253,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_2",
   "Research Artifact": "SCIBERT-MAX-READER",
   "Type": "software",
   "Research Artifact Score": 0.9996197029852619,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9881929830777411,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_0",
   "Research Artifact": "SCIBERT-based models",
   "Type": "software",
   "Research Artifact Score": 0.9993035713265864,
   "Owned": "Yes",
   "Owned Percentage": 100.0,
   "Owned Score": 0.9817036229207657,
   "Reused": "No",
   "Reused Percentage": 0.0,
   "Reused Score": 0.8773231237644786,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_unnamed",
   "Research Artifact": "Unnamed_3",
   "Type": "software",
   "Research Artifact Score": 0.9963321964123784,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9712041584549371,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Kadlec et al., 2016) (33.33333333333333%)\n(Cui et al., 2017) (33.33333333333333%)\n(2018) (33.33333333333333%)",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_unnamed",
   "Research Artifact": "Unnamed_4",
   "Type": "software",
   "Research Artifact Score": 0.9971834064468194,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.993089313489672,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_unnamed",
   "Research Artifact": "Unnamed_5",
   "Type": "software",
   "Research Artifact Score": 0.986126313236547,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9463796214138098,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "",
   "Mentions Count": 1
  },
  {
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "software",
   "Research Artifact Score": 0.9982781115426279,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0,
   "Reused": "Yes",
   "Reused Percentage": 66.66666666666666,
   "Reused Score": 0.9136032587782932,
   "Licenses": "",
   "Versions": "",
   "URLs": "https://www.semanticscholar.org/ (100.0%)",
   "Citations": "(Beltagy et al., 2019) (42.857142857142854%)\n(Devlin et al., 2019) (57.14285714285714%)",
   "Mentions Count": 9
  },
  {
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERTbased",
   "Type": "software",
   "Research Artifact Score": 0.9991471034870838,
   "Owned": "No",
   "Owned Percentage": 0.0,
   "Owned Score": 0.872596297096173,
   "Reused": "Yes",
   "Reused Percentage": 100.0,
   "Reused Score": 0.9835115525766375,
   "Licenses": "",
   "Versions": "",
   "URLs": "",
   "Citations": "(Kadlec et al., 2016) (25.0%)\n(Cui et al., 2017) (25.0%)\n(Devlin et al., 2019) (25.0%)\n(2018) (25.0%)",
   "Mentions Count": 1
  }
 ],
 "mentions": [
  {
   "Mention ID": "C44",
   "RA Cluster": "dataset_cluster_13",
   "Research Artifact": "PUBTATOR",
   "Type": "dataset",
   "Research Artifact Score": 0.9985764992744596,
   "Owned": "No",
   "Owned Score": 0.0006478568965966083,
   "Reused": "Yes",
   "Reused Score": 0.7275873052990344,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Wei et al., 2012)",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    5
   ],
   "Trigger": "repository",
   "Mention": "Unlike BIOREAD, we use PUBTATOR (Wei et al., 2012), a <m>repository</m> that provides approximately 25 million abstracts and their corresponding titles from PUBMED, with multiple annotations. 2"
  },
  {
   "Mention ID": "C49",
   "RA Cluster": "dataset_cluster_12",
   "Research Artifact": "BIOMRC LARGE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9993017750448959,
   "Owned": "No",
   "Owned Score": 0.0005463505936440639,
   "Reused": "Yes",
   "Reused Score": 0.9583852522213184,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    9
   ],
   "Trigger": "BIOMRC",
   "Mention": "Random samples from <m>BIOMRC</m> LARGE where selected to create LITE and TINY."
  },
  {
   "Mention ID": "C73",
   "RA Cluster": "dataset_cluster_12",
   "Research Artifact": "BIOMRC LARGE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9965670912149657,
   "Owned": "No",
   "Owned Score": 0.07615820060978745,
   "Reused": "No",
   "Reused Score": 0.062026576317645445,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Table 2)",
   "Section": "Candidates",
   "Indices": [
    3,
    1,
    8
   ],
   "Trigger": "BIOMRC",
   "Mention": "812k passage-question instances, which form <m>BIOMRC</m> LARGE, split into training, development, and test subsets (Table 2)."
  },
  {
   "Mention ID": "C95",
   "RA Cluster": "dataset_cluster_12",
   "Research Artifact": "BIOMRC LARGE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9910965059830049,
   "Owned": "No",
   "Owned Score": 0.03288680369557282,
   "Reused": "Yes",
   "Reused Score": 0.9164796414309104,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Experiments and Results",
   "Indices": [
    4,
    0,
    2
   ],
   "Trigger": "BIOMRC",
   "Mention": "We hope that others may be able to experiment on <m>BIOMRC</m> LARGE, and we make our code available, as already noted."
  },
  {
   "Mention ID": "C222",
   "RA Cluster": "dataset_cluster_12",
   "Research Artifact": "BIOMRC LARGE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9997203378449169,
   "Owned": "No",
   "Owned Score": 0.0014712201141976375,
   "Reused": "No",
   "Reused Score": 0.15059259980120293,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    2
   ],
   "Trigger": "BIOMRC",
   "Mention": "CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the 812.7k instances of <m>BIOMRC</m> LARGE."
  },
  {
   "Mention ID": "C50",
   "RA Cluster": "dataset_cluster_11",
   "Research Artifact": "BIOMRC TINY",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9951378975886618,
   "Owned": "No",
   "Owned Score": 0.006619178242850535,
   "Reused": "No",
   "Reused Score": 0.18646462273407388,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    10
   ],
   "Trigger": "BIOMRC",
   "Mention": "<m>BIOMRC</m> TINY is used only as a test set; it has no training and validation subsets."
  },
  {
   "Mention ID": "C194",
   "RA Cluster": "dataset_cluster_11",
   "Research Artifact": "BIOMRC TINY",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9988336379744676,
   "Owned": "No",
   "Owned Score": 0.0010954984158356646,
   "Reused": "Yes",
   "Reused Score": 0.8871123176472742,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Passage",
   "Indices": [
    8,
    3,
    3
   ],
   "Trigger": "BIOMRC",
   "Mention": "Table 4 reports the human and system accuracy scores on <m>BIOMRC</m> TINY."
  },
  {
   "Mention ID": "C207",
   "RA Cluster": "dataset_cluster_11",
   "Research Artifact": "BIOMRC TINY",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9982601347109203,
   "Owned": "No",
   "Owned Score": 0.00613660857036979,
   "Reused": "Yes",
   "Reused Score": 0.7995756013848134,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Table 3)",
   "Section": "Passage",
   "Indices": [
    8,
    3,
    10
   ],
   "Trigger": "BIOMRC",
   "Mention": "Also, SCIBERT-MAX-READER performs better than both experts and non-experts in Setting A, and better than non-experts in Setting B. However, <m>BIOMRC</m> TINY contains only 30 instances in each setting, and hence the results of Table 4 are less reliable than those from BIOMRC LITE (Table 3)."
  },
  {
   "Mention ID": "C76",
   "RA Cluster": "dataset_cluster_10",
   "Research Artifact": "CNN | Daily Mail",
   "Type": "dataset",
   "Research Artifact Score": 0.9937967143286052,
   "Owned": "No",
   "Owned Score": 0.00018369764097830862,
   "Reused": "Yes",
   "Reused Score": 0.6851874927003547,
   "License": "N/A | N/A",
   "Version": "N/A | N/A",
   "URLs": "",
   "Citations": "(Hermann et al., 2015)",
   "Section": "Candidates",
   "Indices": [
    3,
    1,
    10
   ],
   "Trigger": "datasets",
   "Mention": "In all versions of BIOMRC (LARGE, LITE, TINY), the entity identifiers of PUBTATOR are replaced by pseudo-identifiers of the form @entityN (Fig. 1), as in the CNN and Daily Mail <m>datasets</m> (Hermann et al., 2015)."
  },
  {
   "Mention ID": "C89",
   "RA Cluster": "dataset_cluster_9",
   "Research Artifact": "MLP",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9418990588677205,
   "Owned": "No",
   "Owned Score": 0.005780830671154391,
   "Reused": "Yes",
   "Reused Score": 0.9439711920455572,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Candidates",
   "Indices": [
    3,
    2,
    2
   ],
   "Trigger": "MLP",
   "Mention": "The top-level embedding produced by SCIBERT for the first sub-token of each candidate answer is concatenated with the toplevel embedding of [MASK] (which replaces the placeholder XXXX) of the question, and they are fed to an <m>MLP</m>, which produces the score of the candidate answer."
  },
  {
   "Mention ID": "C90",
   "RA Cluster": "dataset_cluster_8",
   "Research Artifact": "SCIBERT-SUM-READER",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9939429264961267,
   "Owned": "No",
   "Owned Score": 0.007623180518205094,
   "Reused": "Yes",
   "Reused Score": 0.80524092810768,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Candidates",
   "Indices": [
    3,
    2,
    3
   ],
   "Trigger": "SUM",
   "Mention": "In SCIBERT-<m>SUM</m>-READER, the scores of multiple occurrences of the same candidate are summed, whereas SCIBERT-MAX-READER takes their maximum."
  },
  {
   "Mention ID": "C188",
   "RA Cluster": "dataset_cluster_3",
   "Research Artifact": "BIOMRC LITE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9996525417744626,
   "Owned": "No",
   "Owned Score": 0.017096069573883557,
   "Reused": "Yes",
   "Reused Score": 0.9521754622434232,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Passage",
   "Indices": [
    8,
    3,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "<m>BIOMRC</m> LITE) to three non-experts (graduate CS students) in Setting A, and 30 other questions in Setting B. We also showed the same questions of each setting to two biomedical experts."
  },
  {
   "Mention ID": "C203",
   "RA Cluster": "dataset_cluster_3",
   "Research Artifact": "BIOMRC LITE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9961319220220077,
   "Owned": "No",
   "Owned Score": 0.003105354743036465,
   "Reused": "Yes",
   "Reused Score": 0.9895341599762202,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Passage",
   "Indices": [
    8,
    3,
    9
   ],
   "Trigger": "BIOMRC",
   "Mention": "Unlike our results on <m>BIOMRC</m> LITE, we now see all systems performing better in Setting A compared to Setting B, which suggests they do benefit from the global scope of entity identifiers."
  },
  {
   "Mention ID": "C208",
   "RA Cluster": "dataset_cluster_3",
   "Research Artifact": "BIOMRC LITE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9990593435377778,
   "Owned": "No",
   "Owned Score": 0.0006584084651220794,
   "Reused": "Yes",
   "Reused Score": 0.9939717945117408,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Table 3)",
   "Section": "Passage",
   "Indices": [
    8,
    3,
    10
   ],
   "Trigger": "BIOMRC",
   "Mention": "Also, SCIBERT-MAX-READER performs better than both experts and non-experts in Setting A, and better than non-experts in Setting B. However, BIOMRC TINY contains only 30 instances in each setting, and hence the results of Table 4 are less reliable than those from <m>BIOMRC</m> LITE (Table 3)."
  },
  {
   "Mention ID": "C221",
   "RA Cluster": "dataset_cluster_3",
   "Research Artifact": "BIOMRC LITE",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9981332433513156,
   "Owned": "No",
   "Owned Score": 0.0007589532168390026,
   "Reused": "Yes",
   "Reused Score": 0.6867874588153703,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    2
   ],
   "Trigger": "BIOMRC",
   "Mention": "CLICR contains 100k passage-question instances, the same number as <m>BIOMRC</m> LITE, but much fewer than the 812.7k instances of BIOMRC LARGE."
  },
  {
   "Mention ID": "C201",
   "RA Cluster": "dataset_cluster_6",
   "Research Artifact": "SUM-READER",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9994926243446205,
   "Owned": "No",
   "Owned Score": 0.02381438433062288,
   "Reused": "Yes",
   "Reused Score": 0.9853881070193122,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Passage",
   "Indices": [
    8,
    3,
    7
   ],
   "Trigger": "SUM",
   "Mention": "With sum-aggregation, SCIBERT-<m>SUM</m>-READER obtains exactly the same scores as AOA-READER, which again performs better than AS-READER."
  },
  {
   "Mention ID": "C202",
   "RA Cluster": "dataset_cluster_6",
   "Research Artifact": "SUM-READER",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9976723885768572,
   "Owned": "No",
   "Owned Score": 0.005740623331882259,
   "Reused": "Yes",
   "Reused Score": 0.8749719587336252,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Passage",
   "Indices": [
    8,
    3,
    8
   ],
   "Trigger": "SUM",
   "Mention": "(AOA-READER and SCIBERT-<m>SUM</m>-READER make different mistakes, but their scores just happen to be identical because of the small size of TINY.)"
  },
  {
   "Mention ID": "C217",
   "RA Cluster": "dataset_cluster_5",
   "Research Artifact": "CLICR",
   "Type": "dataset",
   "Research Artifact Score": 0.9860386168204474,
   "Owned": "No",
   "Owned Score": 0.004796625216842912,
   "Reused": "No",
   "Reused Score": 0.46186777612247126,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    1
   ],
   "Trigger": "dataset",
   "Mention": "The closest <m>dataset</m> to ours is CLICR ( \u0160uster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports. 13"
  },
  {
   "Mention ID": "C218",
   "RA Cluster": "dataset_cluster_5",
   "Research Artifact": "CLICR",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9998123629758573,
   "Owned": "No",
   "Owned Score": 0.04849232515483013,
   "Reused": "Yes",
   "Reused Score": 0.723796682694433,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    1
   ],
   "Trigger": "CliCR",
   "Mention": "The closest dataset to ours is <m>CLICR</m> ( \u0160uster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports. 13"
  },
  {
   "Mention ID": "C219",
   "RA Cluster": "dataset_cluster_5",
   "Research Artifact": "CLICR",
   "Type": "dataset",
   "Research Artifact Score": 0.9997777850318023,
   "Owned": "No",
   "Owned Score": 0.36891184160083157,
   "Reused": "No",
   "Reused Score": 0.31477083298680814,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    1
   ],
   "Trigger": "dataset",
   "Mention": "The closest dataset to ours is CLICR ( \u0160uster and Daelemans, 2018), a biomedical MRC <m>dataset</m> with cloze-type questions created using full-text articles from BMJ case reports. 13"
  },
  {
   "Mention ID": "C220",
   "RA Cluster": "dataset_cluster_5",
   "Research Artifact": "CLICR",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9990161710311952,
   "Owned": "No",
   "Owned Score": 0.08959266904643931,
   "Reused": "No",
   "Reused Score": 0.01850117279322949,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    2
   ],
   "Trigger": "CliCR",
   "Mention": "<m>CLICR</m> contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the 812.7k instances of BIOMRC LARGE."
  },
  {
   "Mention ID": "C231",
   "RA Cluster": "dataset_cluster_5",
   "Research Artifact": "CLICR",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9982869017520278,
   "Owned": "No",
   "Owned Score": 0.00035371991723134763,
   "Reused": "No",
   "Reused Score": 0.2255493117508228,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    1,
    4
   ],
   "Trigger": "CliCR",
   "Mention": "It would be particularly interesting to explore if larger automatically generated datasets like BIOMRC and <m>CLICR</m> could be used to pre-train models, which could then be fine-tuned for human-generated QA or MRC datasets."
  },
  {
   "Mention ID": "C223",
   "RA Cluster": "dataset_cluster_4",
   "Research Artifact": "UMLS",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9815927641027737,
   "Owned": "No",
   "Owned Score": 0.0002856748325733932,
   "Reused": "No",
   "Reused Score": 0.18114554387771697,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Lindberg et al., 1993)\n(Soysal et al., 2017)",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    3
   ],
   "Trigger": "UMLS",
   "Mention": "\u0160uster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the <m>UMLS</m> Metathesaurus (Lindberg et al., 1993)."
  },
  {
   "Mention ID": "C224",
   "RA Cluster": "dataset_cluster_1",
   "Research Artifact": "BIOASQ",
   "Type": "dataset",
   "Research Artifact Score": 0.9993108904439872,
   "Owned": "No",
   "Owned Score": 0.001036652034615884,
   "Reused": "No",
   "Reused Score": 0.018787470223189568,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Tsatsaronis et al., 2015)",
   "Section": "Related work",
   "Indices": [
    9,
    1,
    0
   ],
   "Trigger": "dataset",
   "Mention": "The QA <m>dataset</m> of BIOASQ (Tsatsaronis et al., 2015) contains questions written by biomedical experts."
  },
  {
   "Mention ID": "C225",
   "RA Cluster": "dataset_cluster_1",
   "Research Artifact": "BIOASQ",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9990942950139483,
   "Owned": "No",
   "Owned Score": 0.0008637806312461174,
   "Reused": "No",
   "Reused Score": 0.017738562991526416,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Tsatsaronis et al., 2015)",
   "Section": "Related work",
   "Indices": [
    9,
    1,
    0
   ],
   "Trigger": "BioASQ",
   "Mention": "The QA dataset of <m>BIOASQ</m> (Tsatsaronis et al., 2015) contains questions written by biomedical experts."
  },
  {
   "Mention ID": "C228",
   "RA Cluster": "dataset_cluster_1",
   "Research Artifact": "BIOASQ",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9950749398954285,
   "Owned": "Yes",
   "Owned Score": 0.5245999920700455,
   "Reused": "No",
   "Reused Score": 0.003975923594345423,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    1,
    3
   ],
   "Trigger": "BioASQ",
   "Mention": "In the eight years of <m>BIOASQ</m>, only 3,243 questions and gold answers have been created."
  },
  {
   "Mention ID": "C237",
   "RA Cluster": "dataset_cluster_2",
   "Research Artifact": "Google's Natural Questions",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9853322290791022,
   "Owned": "No",
   "Owned Score": 0.00019798746564822736,
   "Reused": "No",
   "Reused Score": 0.014357048206525837,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Kwiatkowski et al., 2019)",
   "Section": "Related work",
   "Indices": [
    9,
    2,
    2
   ],
   "Trigger": "Google",
   "Mention": "To our knowledge the biggest human annotated corpus is <m>Google</m>'s Natural Questions dataset (Kwiatkowski et al., 2019), with approximately 300k human annotated examples."
  },
  {
   "Mention ID": "C239",
   "RA Cluster": "dataset_cluster_2",
   "Research Artifact": "Google's Natural Questions",
   "Type": "dataset",
   "Research Artifact Score": 0.9997314469793662,
   "Owned": "No",
   "Owned Score": 0.0006108266689396138,
   "Reused": "No",
   "Reused Score": 0.01469401760653883,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Kwiatkowski et al., 2019)",
   "Section": "Related work",
   "Indices": [
    9,
    2,
    2
   ],
   "Trigger": "dataset",
   "Mention": "To our knowledge the biggest human annotated corpus is Google's Natural Questions <m>dataset</m> (Kwiatkowski et al., 2019), with approximately 300k human annotated examples."
  },
  {
   "Mention ID": "C238",
   "RA Cluster": "dataset_cluster_0",
   "Research Artifact": "Natural Questions",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.998683488598035,
   "Owned": "No",
   "Owned Score": 0.00018949378958641022,
   "Reused": "No",
   "Reused Score": 0.01798325708194542,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Kwiatkowski et al., 2019)",
   "Section": "Related work",
   "Indices": [
    9,
    2,
    2
   ],
   "Trigger": "Natural Questions",
   "Mention": "To our knowledge the biggest human annotated corpus is Google's <m>Natural Questions</m> dataset (Kwiatkowski et al., 2019), with approximately 300k human annotated examples."
  },
  {
   "Mention ID": "C3",
   "RA Cluster": "dataset_unnamed",
   "Research Artifact": "Unnamed_3",
   "Type": "dataset",
   "Research Artifact Score": 0.9553466336502345,
   "Owned": "Yes",
   "Owned Score": 0.967448113840002,
   "Reused": "No",
   "Reused Score": 0.10757296014336401,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": null,
   "Indices": [
    0,
    0,
    2
   ],
   "Trigger": "dataset",
   "Mention": "Experiments show that simple heuristics do not perform well on the new <m>dataset</m>, and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible."
  },
  {
   "Mention ID": "C2",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOREAD",
   "Type": "dataset",
   "Research Artifact Score": 0.9923875665845089,
   "Owned": "No",
   "Owned Score": 0.0010449644890977953,
   "Reused": "Yes",
   "Reused Score": 0.9056268361130879,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)",
   "Section": null,
   "Indices": [
    0,
    0,
    1
   ],
   "Trigger": "dataset",
   "Mention": "Care was taken to reduce noise, compared to the previous BIOREAD <m>dataset</m> of Pappas et al. (2018)."
  },
  {
   "Mention ID": "C8",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOREAD",
   "Type": "dataset",
   "Research Artifact Score": 0.9981728892332202,
   "Owned": "Yes",
   "Owned Score": 0.9955456564098245,
   "Reused": "Yes",
   "Reused Score": 0.644175018643714,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": null,
   "Indices": [
    0,
    0,
    3
   ],
   "Trigger": "dataset",
   "Mention": "Non-expert human performance is also higher on the new <m>dataset</m> compared to BIOREAD, and biomedical experts perform even better."
  },
  {
   "Mention ID": "C33",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOREAD",
   "Type": "dataset",
   "Research Artifact Score": 0.9983982197015793,
   "Owned": "Yes",
   "Owned Score": 0.9947132096660439,
   "Reused": "No",
   "Reused Score": 0.009542409904131592,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    1,
    5
   ],
   "Trigger": "dataset",
   "Mention": "This allowed Pappas et al. to produce a <m>dataset</m>, called BIOREAD, of approximately 16.4 million questions."
  },
  {
   "Mention ID": "C34",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOREAD",
   "Type": "dataset",
   "Research Artifact Score": 0.9923278812054973,
   "Owned": "No",
   "Owned Score": 0.01102709373724186,
   "Reused": "Yes",
   "Reused Score": 0.8120115284880393,
   "License": "",
   "Version": "",
   "URLs": "https://www.ncbi.nlm.nih.gov/pmc/",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    1,
    7
   ],
   "Trigger": "dataset",
   "Mention": "Although this low score may be due to the fact that the three subjects were not biomedical experts, it is easy to see, by examining samples of BIOREAD, that many examples of the <m>dataset</m> do 1 https://www.ncbi.nlm.nih.gov/pmc/"
  },
  {
   "Mention ID": "C93",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOREAD",
   "Type": "dataset",
   "Research Artifact Score": 0.9976683203786416,
   "Owned": "No",
   "Owned Score": 0.07424572209232282,
   "Reused": "Yes",
   "Reused Score": 0.6263446896996655,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)",
   "Section": "Experiments and Results",
   "Indices": [
    4,
    0,
    1
   ],
   "Trigger": "dataset",
   "Mention": "Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD <m>dataset</m>."
  },
  {
   "Mention ID": "C246",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOREAD",
   "Type": "dataset",
   "Research Artifact Score": 0.9923875665845089,
   "Owned": "No",
   "Owned Score": 0.0010449644890977953,
   "Reused": "Yes",
   "Reused Score": 0.9056268361130879,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    1
   ],
   "Trigger": "dataset",
   "Mention": "Care was taken to reduce noise, compared to the previous BIOREAD <m>dataset</m> of Pappas et al. (2018)."
  },
  {
   "Mention ID": "C28",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "MRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9958951246765908,
   "Owned": "Yes",
   "Owned Score": 0.9965117699052424,
   "Reused": "No",
   "Reused Score": 0.25345554238270973,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)",
   "Section": "Introduction",
   "Indices": [
    1,
    1,
    0
   ],
   "Trigger": "dataset",
   "Mention": "To bypass the need for expert annotators and produce a biomedical MRC <m>dataset</m> large enough to train (or pre-train) deep learning models, Pappas et al. (2018) adopted the cloze-style questions approach."
  },
  {
   "Mention ID": "C235",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "MRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9092847760626837,
   "Owned": "No",
   "Owned Score": 0.0016499816635561176,
   "Reused": "No",
   "Reused Score": 0.006632742765698835,
   "License": "publicly available",
   "Version": "",
   "URLs": "",
   "Citations": "(Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)",
   "Section": "Related work",
   "Indices": [
    9,
    2,
    1
   ],
   "Trigger": "datasets",
   "Mention": "There are also several large open-domain MRC <m>datasets</m> annotated by humans (Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)."
  },
  {
   "Mention ID": "C0",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.999828938291187,
   "Owned": "Yes",
   "Owned Score": 0.9953636619451637,
   "Reused": "No",
   "Reused Score": 0.008859232032890374,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": null,
   "Indices": [
    0,
    0,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "We introduce <m>BIOMRC</m>, a large-scale clozestyle biomedical MRC dataset."
  },
  {
   "Mention ID": "C1",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9995994758385122,
   "Owned": "Yes",
   "Owned Score": 0.9280012745602934,
   "Reused": "No",
   "Reused Score": 0.010632268850794074,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": null,
   "Indices": [
    0,
    0,
    0
   ],
   "Trigger": "dataset",
   "Mention": "We introduce BIOMRC, a large-scale clozestyle biomedical MRC <m>dataset</m>."
  },
  {
   "Mention ID": "C5",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9978609599383274,
   "Owned": "No",
   "Owned Score": 0.1759959540483707,
   "Reused": "Yes",
   "Reused Score": 0.9563234611767467,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": null,
   "Indices": [
    0,
    0,
    2
   ],
   "Trigger": "BIOMRC",
   "Mention": "Experiments show that simple heuristics do not perform well on the new dataset, and that two neural MRC models that had been tested on BIOREAD perform much better on <m>BIOMRC</m>, indicating that the new dataset is indeed less noisy or at least that its task is more feasible."
  },
  {
   "Mention ID": "C6",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9783235399124389,
   "Owned": "Yes",
   "Owned Score": 0.9835706776802967,
   "Reused": "Yes",
   "Reused Score": 0.6920042136346333,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": null,
   "Indices": [
    0,
    0,
    2
   ],
   "Trigger": "dataset",
   "Mention": "Experiments show that simple heuristics do not perform well on the new dataset, and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new <m>dataset</m> is indeed less noisy or at least that its task is more feasible."
  },
  {
   "Mention ID": "C36",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9997838032829774,
   "Owned": "Yes",
   "Owned Score": 0.9996217803957638,
   "Reused": "No",
   "Reused Score": 0.013350062509517317,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "In this paper, we introduce <m>BIOMRC</m>, a new dataset for biomedical MRC that can be viewed as an improved version of BIOREAD."
  },
  {
   "Mention ID": "C37",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9992284957655034,
   "Owned": "Yes",
   "Owned Score": 0.9994645621813737,
   "Reused": "No",
   "Reused Score": 0.02010954120889501,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    0
   ],
   "Trigger": "dataset",
   "Mention": "In this paper, we introduce BIOMRC, a new <m>dataset</m> for biomedical MRC that can be viewed as an improved version of BIOREAD."
  },
  {
   "Mention ID": "C48",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.999818871765646,
   "Owned": "Yes",
   "Owned Score": 0.9232104305854221,
   "Reused": "No",
   "Reused Score": 0.09267616536465764,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    8
   ],
   "Trigger": "BIOMRC",
   "Mention": "Following Pappas et al. (2018), we release two versions of <m>BIOMRC</m>, LARGE and LITE, containing 812k and 100k instances respectively, for researchers with more or fewer resources, along with the 60 instances (TINY) humans answered."
  },
  {
   "Mention ID": "C67",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9802379913270346,
   "Owned": "No",
   "Owned Score": 0.0047576620570434994,
   "Reused": "No",
   "Reused Score": 0.25159202899911365,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Candidates",
   "Indices": [
    3,
    1,
    0
   ],
   "Trigger": "INSTANCE",
   "Mention": "Figure 1: Example passage-question <m>instance</m> of BIOMRC."
  },
  {
   "Mention ID": "C68",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.998489072160623,
   "Owned": "No",
   "Owned Score": 0.00568742534831189,
   "Reused": "No",
   "Reused Score": 0.09761421981080011,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Candidates",
   "Indices": [
    3,
    1,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "Figure 1: Example passage-question instance of <m>BIOMRC</m>."
  },
  {
   "Mention ID": "C74",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9990182132158456,
   "Owned": "No",
   "Owned Score": 0.0015116048288895557,
   "Reused": "No",
   "Reused Score": 0.2349573838039722,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Candidates",
   "Indices": [
    3,
    1,
    9
   ],
   "Trigger": "BIOMRC",
   "Mention": "The LITE and TINY versions of <m>BIOMRC</m> are subsets of LARGE."
  },
  {
   "Mention ID": "C75",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9860723068150449,
   "Owned": "No",
   "Owned Score": 0.0040884426538919435,
   "Reused": "No",
   "Reused Score": 0.33881209270685175,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Hermann et al., 2015)",
   "Section": "Candidates",
   "Indices": [
    3,
    1,
    10
   ],
   "Trigger": "BIOMRC",
   "Mention": "In all versions of <m>BIOMRC</m> (LARGE, LITE, TINY), the entity identifiers of PUBTATOR are replaced by pseudo-identifiers of the form @entityN (Fig. 1), as in the CNN and Daily Mail datasets (Hermann et al., 2015)."
  },
  {
   "Mention ID": "C77",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9981488581053012,
   "Owned": "No",
   "Owned Score": 0.2302665092980406,
   "Reused": "Yes",
   "Reused Score": 0.7619691449829188,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)",
   "Section": "Candidates",
   "Indices": [
    3,
    1,
    11
   ],
   "Trigger": "BIOMRC",
   "Mention": "We provide all <m>BIOMRC</m> versions in two forms, corresponding to what Pappas et al.  (2018) call Settings A and B in BIOREAD. 6"
  },
  {
   "Mention ID": "C85",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9942073731114925,
   "Owned": "No",
   "Owned Score": 0.004631334261828295,
   "Reused": "No",
   "Reused Score": 0.09464073335667106,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Candidates",
   "Indices": [
    3,
    2,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "Table 2 provides statistics on <m>BIOMRC</m>."
  },
  {
   "Mention ID": "C91",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9989027847130028,
   "Owned": "No",
   "Owned Score": 0.00034891135896029886,
   "Reused": "Yes",
   "Reused Score": 0.9338099938676598,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Experiments and Results",
   "Indices": [
    4,
    0,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "We experimented only on <m>BIOMRC</m> LITE and TINY, since we did not have the computational resources to train the neural models we considered on the LARGE version of BIOREAD."
  },
  {
   "Mention ID": "C216",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9996967337093402,
   "Owned": "No",
   "Owned Score": 0.002210730847621816,
   "Reused": "No",
   "Reused Score": 0.00994405542665384,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Pampari et al., 2018;Ben Abacha et al., 2019;Zhang et al., 2018)",
   "Section": "Related work",
   "Indices": [
    9,
    0,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than <m>BIOMRC</m> (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018;Ben Abacha et al., 2019;Zhang et al., 2018)."
  },
  {
   "Mention ID": "C230",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9986905009511478,
   "Owned": "No",
   "Owned Score": 0.0004987363199738744,
   "Reused": "No",
   "Reused Score": 0.055781567844701285,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Related work",
   "Indices": [
    9,
    1,
    4
   ],
   "Trigger": "BIOMRC",
   "Mention": "It would be particularly interesting to explore if larger automatically generated datasets like <m>BIOMRC</m> and CLICR could be used to pre-train models, which could then be fine-tuned for human-generated QA or MRC datasets."
  },
  {
   "Mention ID": "C244",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9998148972149432,
   "Owned": "Yes",
   "Owned Score": 0.9972716694630833,
   "Reused": "No",
   "Reused Score": 0.011647391360945858,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    0
   ],
   "Trigger": "BIOMRC",
   "Mention": "We introduced <m>BIOMRC</m>, a large-scale cloze-style biomedical MRC dataset."
  },
  {
   "Mention ID": "C245",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9995156670738751,
   "Owned": "Yes",
   "Owned Score": 0.9734142404750958,
   "Reused": "No",
   "Reused Score": 0.016934153161590713,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    0
   ],
   "Trigger": "dataset",
   "Mention": "We introduced BIOMRC, a large-scale cloze-style biomedical MRC <m>dataset</m>."
  },
  {
   "Mention ID": "C247",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9828251031561739,
   "Owned": "Yes",
   "Owned Score": 0.52686927507249,
   "Reused": "No",
   "Reused Score": 0.3779894600120342,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    2
   ],
   "Trigger": "BIOMRC",
   "Mention": "Experiments showed that <m>BIOMRC</m>'s questions cannot be answered well by simple heuristics, and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible."
  },
  {
   "Mention ID": "C249",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.997128628151922,
   "Owned": "No",
   "Owned Score": 0.33110830061279733,
   "Reused": "Yes",
   "Reused Score": 0.9832036436530958,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    2
   ],
   "Trigger": "BIOMRC",
   "Mention": "Experiments showed that BIOMRC's questions cannot be answered well by simple heuristics, and that two neural MRC models that had been tested on BIOREAD perform much better on <m>BIOMRC</m>, indicating that the new dataset is indeed less noisy or at least that its task is more feasible."
  },
  {
   "Mention ID": "C250",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "dataset",
   "Research Artifact Score": 0.9831396263510862,
   "Owned": "Yes",
   "Owned Score": 0.9820987458805397,
   "Reused": "Yes",
   "Reused Score": 0.8545698669059993,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    2
   ],
   "Trigger": "dataset",
   "Mention": "Experiments showed that BIOMRC's questions cannot be answered well by simple heuristics, and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new <m>dataset</m> is indeed less noisy or at least that its task is more feasible."
  },
  {
   "Mention ID": "C251",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9998814702701754,
   "Owned": "No",
   "Owned Score": 0.020754167185059253,
   "Reused": "Yes",
   "Reused Score": 0.9268568343672408,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    3
   ],
   "Trigger": "BIOMRC",
   "Mention": "Human performance was also higher on a sample of <m>BIOMRC</m> compared to BIOREAD, and biomedical experts performed even better."
  },
  {
   "Mention ID": "C254",
   "RA Cluster": "dataset_cluster_15_16_17",
   "Research Artifact": "BIOMRC",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9903856632926828,
   "Owned": "No",
   "Owned Score": 0.3801814682874642,
   "Reused": "Yes",
   "Reused Score": 0.573748096129505,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    0,
    5
   ],
   "Trigger": "BIOMRC",
   "Mention": "We make <m>BIOMRC</m> available in three different sizes, also releasing our code, and providing a leaderboard."
  },
  {
   "Mention ID": "C31",
   "RA Cluster": "dataset_cluster_7_14",
   "Research Artifact": "PUBMED CENTRAL",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9980421370969207,
   "Owned": "No",
   "Owned Score": 0.00022960328766164084,
   "Reused": "Yes",
   "Reused Score": 0.9537291994702725,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    1,
    1
   ],
   "Trigger": "Pubmed",
   "Mention": "They used the full text of unlabeled biomedical articles from <m>PUBMED</m> CENTRAL, 1 and METAMAP (Aronson and Lang, 2010) to annotate the biomedical entities of the articles."
  },
  {
   "Mention ID": "C40",
   "RA Cluster": "dataset_cluster_7_14",
   "Research Artifact": "PUBMED",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9386061054695645,
   "Owned": "No",
   "Owned Score": 0.002062362316833443,
   "Reused": "Yes",
   "Reused Score": 0.9666058353151737,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    1
   ],
   "Trigger": "Pubmed",
   "Mention": "To avoid crossing sections, extracting text from references, captions, tables etc., we use abstracts and titles of biomedical articles as passages and questions, respectively, which are clearly marked up in <m>PUBMED</m> data, instead of using the full text of the articles."
  },
  {
   "Mention ID": "C45",
   "RA Cluster": "dataset_cluster_7_14",
   "Research Artifact": "PUBMED",
   "Type": "gaz_dataset",
   "Research Artifact Score": 0.9910729485475325,
   "Owned": "No",
   "Owned Score": 0.0011286770562090392,
   "Reused": "Yes",
   "Reused Score": 0.6086359704749542,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Wei et al., 2012)",
   "Section": "Introduction",
   "Indices": [
    1,
    3,
    5
   ],
   "Trigger": "Pubmed",
   "Mention": "Unlike BIOREAD, we use PUBTATOR (Wei et al., 2012), a repository that provides approximately 25 million abstracts and their corresponding titles from <m>PUBMED</m>, with multiple annotations. 2"
  },
  {
   "Mention ID": "C52",
   "RA Cluster": "software_cluster_16",
   "Research Artifact": "Attention Sum Reader",
   "Type": "software",
   "Research Artifact Score": 0.9995643310682689,
   "Owned": "No",
   "Owned Score": 0.00510441313423879,
   "Reused": "Yes",
   "Reused Score": 0.9860577199711463,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)\n(Cui et al., 2017)\n(Kadlec et al., 2016)",
   "Section": "Introduction",
   "Indices": [
    1,
    4,
    0
   ],
   "Trigger": "models",
   "Mention": "We tested on BIOMRC LITE the two deep learning MRC <m>models</m> that Pappas et al. (2018) had tested on BIOREAD LITE, namely Attention Sum Reader (AS-READER) (Kadlec et al., 2016) and Attention Over Attention Reader (AOA-READER) (Cui et al., 2017)."
  },
  {
   "Mention ID": "C56",
   "RA Cluster": "software_cluster_15",
   "Research Artifact": "MRC",
   "Type": "software",
   "Research Artifact Score": 0.9995238420628769,
   "Owned": "Yes",
   "Owned Score": 0.9969517176786158,
   "Reused": "Yes",
   "Reused Score": 0.8766359215456292,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Devlin et al., 2019)",
   "Section": "Introduction",
   "Indices": [
    1,
    4,
    2
   ],
   "Trigger": "model",
   "Mention": "We also developed a new BERTbased (Devlin et al., 2019) MRC <m>model</m>, the best version of which (SCIBERT-MAX-READER) performs even better, with its accuracy reaching 80%."
  },
  {
   "Mention ID": "C96",
   "RA Cluster": "software_cluster_7",
   "Research Artifact": "neural MRC",
   "Type": "software",
   "Research Artifact Score": 0.9991310550262936,
   "Owned": "No",
   "Owned Score": 0.003317158648696739,
   "Reused": "Yes",
   "Reused Score": 0.9823642384926757,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)\n(Cui et al., 2017)\n(Devlin et al., 2019)\n(Kadlec et al., 2016)",
   "Section": "Methods",
   "Indices": [
    5,
    0,
    0
   ],
   "Trigger": "models",
   "Mention": "We experimented with the four basic baselines (BASE1-4) that Pappas et al. (2018) used in BIOREAD, the two neural MRC <m>models</m> used by the same authors, AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) model we developed."
  },
  {
   "Mention ID": "C100",
   "RA Cluster": "software_cluster_13",
   "Research Artifact": "N/A | N/A",
   "Type": "software",
   "Research Artifact Score": 0.9923905271043041,
   "Owned": "No",
   "Owned Score": 0.001760271977441026,
   "Reused": "Yes",
   "Reused Score": 0.8729217545803231,
   "License": "N/A | N/A",
   "Version": "N/A | N/A",
   "URLs": "",
   "Citations": "(2018)\n(Cui et al., 2017)\n(Kadlec et al., 2016)",
   "Section": "Methods",
   "Indices": [
    5,
    2,
    0
   ],
   "Trigger": "models",
   "Mention": "Neural baselines: We use the same implementations of AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural <m>models</m>, not provided here to save space."
  },
  {
   "Mention ID": "C105",
   "RA Cluster": "software_cluster_11",
   "Research Artifact": "BERT-based model",
   "Type": "software",
   "Research Artifact Score": 0.9979614751005492,
   "Owned": "No",
   "Owned Score": 0.004389153468317399,
   "Reused": "Yes",
   "Reused Score": 0.9569080256526209,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Beltagy et al., 2019)\n(Devlin et al., 2019)",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    0
   ],
   "Trigger": "model",
   "Mention": "BERT-based <m>model</m>: We use SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin et al., 2019) model for scientific text."
  },
  {
   "Mention ID": "C106",
   "RA Cluster": "software_cluster_10",
   "Research Artifact": "SCIBERT",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9996811067767043,
   "Owned": "Yes",
   "Owned Score": 0.9097216700698656,
   "Reused": "Yes",
   "Reused Score": 0.9874375463187011,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Beltagy et al., 2019)\n(Devlin et al., 2019)",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    0
   ],
   "Trigger": "USE",
   "Mention": "BERT-based model: We <m>use</m> SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin et al., 2019) model for scientific text."
  },
  {
   "Mention ID": "C146",
   "RA Cluster": "software_cluster_10",
   "Research Artifact": "SCIBERT",
   "Type": "software",
   "Research Artifact Score": 0.9898919213692141,
   "Owned": "No",
   "Owned Score": 0.005943792751616978,
   "Reused": "Yes",
   "Reused Score": 0.9363525608333926,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    0,
    4
   ],
   "Trigger": "models",
   "Mention": "We expect, however, that the performance of the SCIBERT-based <m>models</m>, could be improved further by fine-tuning SCIBERT's parameters."
  },
  {
   "Mention ID": "C153",
   "RA Cluster": "software_cluster_10",
   "Research Artifact": "SCIBERT",
   "Type": "software",
   "Research Artifact Score": 0.9983583185248929,
   "Owned": "No",
   "Owned Score": 0.06286822864365205,
   "Reused": "Yes",
   "Reused Score": 0.991568631709088,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    2,
    1
   ],
   "Trigger": "models",
   "Mention": "he two SCIBERT-based <m>models</m> are also competitive in terms of training time, because we only train the MLP (154k parameters) on top of SCIB-ERT, keeping the parameters of SCIBERT frozen."
  },
  {
   "Mention ID": "C161",
   "RA Cluster": "software_cluster_10",
   "Research Artifact": "SCIBERT",
   "Type": "software",
   "Research Artifact Score": 0.9971612846001509,
   "Owned": "No",
   "Owned Score": 0.006992986535310124,
   "Reused": "Yes",
   "Reused Score": 0.9568394208000587,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    3,
    3
   ],
   "Trigger": "models",
   "Mention": "Note that the hyper-parameters of the two SCIBERT-based <m>models</m> (of their MLPs) were very minimally tuned, hence these models may perform even better with more extensive tuning."
  },
  {
   "Mention ID": "C162",
   "RA Cluster": "software_cluster_10",
   "Research Artifact": "SCIBERT",
   "Type": "software",
   "Research Artifact Score": 0.9976631279149281,
   "Owned": "No",
   "Owned Score": 0.05733818374259015,
   "Reused": "Yes",
   "Reused Score": 0.9740493926173016,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    3,
    3
   ],
   "Trigger": "models",
   "Mention": "Note that the hyper-parameters of the two SCIBERT-based models (of their MLPs) were very minimally tuned, hence these <m>models</m> may perform even better with more extensive tuning."
  },
  {
   "Mention ID": "C117",
   "RA Cluster": "software_cluster_9",
   "Research Artifact": "softmax",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9549422460788782,
   "Owned": "No",
   "Owned Score": 0.19335163911018755,
   "Reused": "Yes",
   "Reused Score": 0.9744656856558018,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    8
   ],
   "Trigger": "Softmax",
   "Mention": "In both cases, a <m>softmax</m> is then applied to the scores of all the entities, and the entity with the maximum score is selected as the answer."
  },
  {
   "Mention ID": "C118",
   "RA Cluster": "software_cluster_8",
   "Research Artifact": "BIOMRC LARGE",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9931787901325224,
   "Owned": "No",
   "Owned Score": 0.05003683924920745,
   "Reused": "Yes",
   "Reused Score": 0.9924830117122081,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    10
   ],
   "Trigger": "USE",
   "Mention": ", 6 and <m>use</m> n = 3, which gave the best accuracy on the development set of BIOMRC LARGE."
  },
  {
   "Mention ID": "C127",
   "RA Cluster": "software_cluster_3",
   "Research Artifact": "SCIBERT-SUM-READER",
   "Type": "software",
   "Research Artifact Score": 0.9989786210627868,
   "Owned": "No",
   "Owned Score": 0.06194579805807388,
   "Reused": "Yes",
   "Reused Score": 0.819995440861383,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Methods",
   "Indices": [
    5,
    5,
    0
   ],
   "Trigger": "model",
   "Mention": "this <m>model</m> SCIBERT-SUM-READER or SCIBERT-MAX-READER, depending on how it aggregates the scores of multiple occurrences of the same entity."
  },
  {
   "Mention ID": "C148",
   "RA Cluster": "software_cluster_3",
   "Research Artifact": "SCIBERT-SUM-READER",
   "Type": "software",
   "Research Artifact Score": 0.9973304187922784,
   "Owned": "No",
   "Owned Score": 0.06503601859130645,
   "Reused": "Yes",
   "Reused Score": 0.9772821064284825,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    1,
    0
   ],
   "Trigger": "model",
   "Mention": "The performance of SCIBERT-SUM-READER is slightly better in Setting A than in Setting B, which might suggest that the <m>model</m> manages to capture global properties of the entity pseudo-identifiers from the entire training set."
  },
  {
   "Mention ID": "C136",
   "RA Cluster": "software_cluster_6",
   "Research Artifact": "BIOMRC LITE",
   "Type": "software",
   "Research Artifact Score": 0.99412849961018,
   "Owned": "No",
   "Owned Score": 0.03212682205320531,
   "Reused": "Yes",
   "Reused Score": 0.9890215013761572,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    0,
    0
   ],
   "Trigger": "methods",
   "Mention": "Table 3 reports the accuracy of all <m>methods</m> on BIOMRC LITE for Settings A and B. In both settings, all the neural models clearly outperform all the basic baselines, with BASE3 (most frequent entity of the passage) performing worst and BASE3+ performing much better, as expected."
  },
  {
   "Mention ID": "C138",
   "RA Cluster": "software_cluster_6",
   "Research Artifact": "BIOMRC LITE",
   "Type": "software",
   "Research Artifact Score": 0.9951862948433927,
   "Owned": "No",
   "Owned Score": 0.039963330378681806,
   "Reused": "Yes",
   "Reused Score": 0.9867092828949922,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    0,
    0
   ],
   "Trigger": "models",
   "Mention": "Table 3 reports the accuracy of all methods on BIOMRC LITE for Settings A and B. In both settings, all the neural <m>models</m> clearly outperform all the basic baselines, with BASE3 (most frequent entity of the passage) performing worst and BASE3+ performing much better, as expected."
  },
  {
   "Mention ID": "C149",
   "RA Cluster": "software_cluster_5",
   "Research Artifact": "AS-READER | AOA-READER",
   "Type": "software",
   "Research Artifact Score": 0.9969126363271814,
   "Owned": "No",
   "Owned Score": 0.003188323331383084,
   "Reused": "Yes",
   "Reused Score": 0.8893710118139884,
   "License": "N/A | N/A",
   "Version": "N/A | N/A",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    1,
    2
   ],
   "Trigger": "models",
   "Mention": "Furthermore, the development and test performance of AS-READER and AOA-READER is higher in Setting B than A, indicating that these two <m>models</m> do not capture global properties of entities well, performing better when forced to consider only the information of the particular passage-question instance."
  },
  {
   "Mention ID": "C155",
   "RA Cluster": "software_cluster_5",
   "Research Artifact": "AS-READER | AOA-READER",
   "Type": "software",
   "Research Artifact Score": 0.9970462399469884,
   "Owned": "No",
   "Owned Score": 0.004864694934363225,
   "Reused": "Yes",
   "Reused Score": 0.9711352978021826,
   "License": "N/A | N/A",
   "Version": "N/A | N/A",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    3,
    0
   ],
   "Trigger": "models",
   "Mention": "The trainable parameters of AS-READER and AOA-READER are almost double in Setting A compared to Setting B. To some extent, this difference is due to the fact that for both <m>models</m> we learn a word embedding for each @entityN pseudoidentifier, and in Setting A the numbering of the identifiers is not reset for each passage-question instance, leading to many more pseudo-identifiers (31.77k pseudo-identifiers in the vocabulary of Setting A vs. only 20 in Setting B); this accounts for a difference of 1.59M parameters. 11"
  },
  {
   "Mention ID": "C152",
   "RA Cluster": "software_cluster_4",
   "Research Artifact": "AOA-READER",
   "Type": "software",
   "Research Artifact Score": 0.9335151552816192,
   "Owned": "No",
   "Owned Score": 0.052957401625467394,
   "Reused": "Yes",
   "Reused Score": 0.8787890998437556,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Table 3)",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    2,
    0
   ],
   "Trigger": "mechanism",
   "Mention": "In both Settings A and B, AOA-READER performs better than AS-READER, which was expected since it uses a more elaborate attention <m>mechanism</m>, at the expense of taking longer to train (Table 3). 10"
  },
  {
   "Mention ID": "C169",
   "RA Cluster": "software_cluster_1",
   "Research Artifact": "neural",
   "Type": "software",
   "Research Artifact Score": 0.9132799423787455,
   "Owned": "No",
   "Owned Score": 0.00110137610542192,
   "Reused": "Yes",
   "Reused Score": 0.8905575462620253,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    3,
    10
   ],
   "Trigger": "models",
   "Mention": "BASE3+ is the best basic baseline for 2 and 3 candidates, and for 2 candidates it is competitive to the neural <m>models</m>."
  },
  {
   "Mention ID": "C171",
   "RA Cluster": "software_cluster_2",
   "Research Artifact": "SCIBERT-MAX-READER",
   "Type": "software",
   "Research Artifact Score": 0.9996197029852619,
   "Owned": "No",
   "Owned Score": 0.08217604604768235,
   "Reused": "Yes",
   "Reused Score": 0.9881929830777411,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Results on BIOMRC LITE",
   "Indices": [
    6,
    3,
    11
   ],
   "Trigger": "system",
   "Mention": "Overall, however, BASE4 is clearly the best basic baseline, but it is outperformed by all neural models in almost all cases, as in Table 3. SCIBERT-MAX-READER is again the best <m>system</m> in both settings, almost always outperforming the other systems."
  },
  {
   "Mention ID": "C262",
   "RA Cluster": "software_cluster_0",
   "Research Artifact": "SCIBERT-based models",
   "Type": "software",
   "Research Artifact Score": 0.9993035713265864,
   "Owned": "Yes",
   "Owned Score": 0.9817036229207657,
   "Reused": "Yes",
   "Reused Score": 0.8773231237644786,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Figure 2 :",
   "Indices": [
    11,
    0,
    0
   ],
   "Trigger": "models",
   "Mention": "Figure 2: Illustration of our SCIBERT-based <m>models</m>.Each sentence of the passage is concatenated with the question and fed to SCIBERT."
  },
  {
   "Mention ID": "C99",
   "RA Cluster": "software_unnamed",
   "Research Artifact": "Unnamed_3",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9963321964123784,
   "Owned": "No",
   "Owned Score": 0.03191507423484812,
   "Reused": "Yes",
   "Reused Score": 0.9712041584549371,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)\n(Cui et al., 2017)\n(Kadlec et al., 2016)",
   "Section": "Methods",
   "Indices": [
    5,
    2,
    0
   ],
   "Trigger": "USE",
   "Mention": "Neural baselines: We <m>use</m> the same implementations of AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space."
  },
  {
   "Mention ID": "C102",
   "RA Cluster": "software_unnamed",
   "Research Artifact": "Unnamed_4",
   "Type": "software",
   "Research Artifact Score": 0.9971834064468194,
   "Owned": "No",
   "Owned Score": 0.02204172033691773,
   "Reused": "Yes",
   "Reused Score": 0.993089313489672,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Methods",
   "Indices": [
    5,
    2,
    1
   ],
   "Trigger": "methods",
   "Mention": "The hyper-parameters of both <m>methods</m> were tuned on the development set of BIOMRC LITE."
  },
  {
   "Mention ID": "C86",
   "RA Cluster": "software_unnamed",
   "Research Artifact": "Unnamed_5",
   "Type": "gaz_method",
   "Research Artifact Score": 0.986126313236547,
   "Owned": "No",
   "Owned Score": 0.31481284994027353,
   "Reused": "Yes",
   "Reused Score": 0.9463796214138098,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Candidates",
   "Indices": [
    3,
    2,
    1
   ],
   "Trigger": "USE",
   "Mention": "In TINY, we <m>use</m> 30 different passage-question instances in Settings A and B, because in both settings we asked the same humans to answer the questions, and we Each sentence of the passage is concatenated with the question and fed to SCIBERT."
  },
  {
   "Mention ID": "C104",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9998114705419856,
   "Owned": "No",
   "Owned Score": 0.004355362076078708,
   "Reused": "Yes",
   "Reused Score": 0.974506623147584,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Beltagy et al., 2019)\n(Devlin et al., 2019)",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    0
   ],
   "Trigger": "BERT",
   "Mention": "<m>BERT</m>-based model: We use SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin et al., 2019) model for scientific text."
  },
  {
   "Mention ID": "C107",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9997186697470738,
   "Owned": "No",
   "Owned Score": 0.004694376887721159,
   "Reused": "Yes",
   "Reused Score": 0.9908179991281727,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Beltagy et al., 2019)\n(Devlin et al., 2019)",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    0
   ],
   "Trigger": "BERT",
   "Mention": "BERT-based model: We use SCIBERT (Beltagy et al., 2019), a pre-trained <m>BERT</m> (Devlin et al., 2019) model for scientific text."
  },
  {
   "Mention ID": "C108",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "software",
   "Research Artifact Score": 0.9998215447684585,
   "Owned": "No",
   "Owned Score": 0.0016287328478407994,
   "Reused": "Yes",
   "Reused Score": 0.9908817410203428,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(Beltagy et al., 2019)\n(Devlin et al., 2019)",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    0
   ],
   "Trigger": "model",
   "Mention": "BERT-based model: We use SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin et al., 2019) <m>model</m> for scientific text."
  },
  {
   "Mention ID": "C113",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9958316151085611,
   "Owned": "No",
   "Owned Score": 0.0020267707616910048,
   "Reused": "Yes",
   "Reused Score": 0.9890671781949489,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    3
   ],
   "Trigger": "BERT",
   "Mention": "For each sentence, we concatenate it (using <m>BERT</m>'s [SEP] token) with the question, after replacing the XXXX with BERT's [MASK] token, and we feed the concatenation to SCIBERT (Fig. 2)."
  },
  {
   "Mention ID": "C114",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9973042568951251,
   "Owned": "No",
   "Owned Score": 0.005196151768085362,
   "Reused": "Yes",
   "Reused Score": 0.9769472437565985,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Methods",
   "Indices": [
    5,
    3,
    3
   ],
   "Trigger": "BERT",
   "Mention": "For each sentence, we concatenate it (using BERT's [SEP] token) with the question, after replacing the XXXX with <m>BERT</m>'s [MASK] token, and we feed the concatenation to SCIBERT (Fig. 2)."
  },
  {
   "Mention ID": "C120",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "gaz_method",
   "Research Artifact Score": 0.996461577072785,
   "Owned": "No",
   "Owned Score": 0.0016712975247746237,
   "Reused": "No",
   "Reused Score": 0.16476937148171808,
   "License": "",
   "Version": "",
   "URLs": "https://www.semanticscholar.org/",
   "Citations": "(Devlin et al., 2019)",
   "Section": "Methods",
   "Indices": [
    5,
    4,
    0
   ],
   "Trigger": "BERT",
   "Mention": "8 https://www.semanticscholar.org/ 9 <m>BERT</m>'s tokenizer splits the entity identifiers into subtokens (Devlin et al., 2019)."
  },
  {
   "Mention ID": "C122",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "gaz_method",
   "Research Artifact Score": 0.9979714937503432,
   "Owned": "No",
   "Owned Score": 0.00817328405867065,
   "Reused": "No",
   "Reused Score": 0.3492508502884087,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Methods",
   "Indices": [
    5,
    4,
    2
   ],
   "Trigger": "BERT",
   "Mention": "The top-level token representations of <m>BERT</m> are context-aware, and it is common to use the first or last sub-token of each named-entity."
  },
  {
   "Mention ID": "C255",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "software",
   "Research Artifact Score": 0.9983499626242819,
   "Owned": "No",
   "Owned Score": 0.47905483056806597,
   "Reused": "Yes",
   "Reused Score": 0.54922930416227,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    1,
    0
   ],
   "Trigger": "model",
   "Mention": "We plan to tune more extensively the BERTbased <m>model</m> to further improve its efficiency, and to investigate if some of its techniques (mostly its max-aggregation, but also using sub-tokens) can also benefit the other neural models we considered."
  },
  {
   "Mention ID": "C256",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERT",
   "Type": "software",
   "Research Artifact Score": 0.9992324133750349,
   "Owned": "No",
   "Owned Score": 0.10286684986809844,
   "Reused": "Yes",
   "Reused Score": 0.9237727220381354,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "",
   "Section": "Conclusions and Future Work",
   "Indices": [
    10,
    1,
    0
   ],
   "Trigger": "techniques",
   "Mention": "We plan to tune more extensively the BERTbased model to further improve its efficiency, and to investigate if some of its <m>techniques</m> (mostly its max-aggregation, but also using sub-tokens) can also benefit the other neural models we considered."
  },
  {
   "Mention ID": "C97",
   "RA Cluster": "software_cluster_12_14",
   "Research Artifact": "BERTbased",
   "Type": "software",
   "Research Artifact Score": 0.9991471034870838,
   "Owned": "Yes",
   "Owned Score": 0.872596297096173,
   "Reused": "Yes",
   "Reused Score": 0.9835115525766375,
   "License": "",
   "Version": "",
   "URLs": "",
   "Citations": "(2018)\n(Cui et al., 2017)\n(Devlin et al., 2019)\n(Kadlec et al., 2016)",
   "Section": "Methods",
   "Indices": [
    5,
    0,
    0
   ],
   "Trigger": "model",
   "Mention": "We experimented with the four basic baselines (BASE1-4) that Pappas et al. (2018) used in BIOREAD, the two neural MRC models used by the same authors, AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) <m>model</m> we developed."
  }
 ],
 "citations": [
  {
   "Id": "b0",
   "Doi": "10.1136/jamia.2009.002733",
   "Title": "An overview of MetaMap: historical perspective and recent advances",
   "Authors": "Alan Aronson\nFran\u00e7ois-Michel Lang"
  },
  {
   "Id": "b1",
   "Doi": "",
   "Title": "",
   "Authors": ""
  },
  {
   "Id": "b2",
   "Doi": "10.18653/v1/d19-1371",
   "Title": "SciBERT: A Pretrained Language Model for Scientific Text",
   "Authors": "Iz Beltagy\nKyle Lo\nArman Cohan"
  },
  {
   "Id": "b3",
   "Doi": "",
   "Title": "A question-entailment approach to question answering",
   "Authors": "Asma Ben\nAbacha \nDina Demner-Fushman"
  },
  {
   "Id": "b4",
   "Doi": "10.18653/v1/w19-5039",
   "Title": "Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering",
   "Authors": "Asma Ben Abacha\nChaitanya Shivade\nDina Demner-Fushman"
  },
  {
   "Id": "b5",
   "Doi": "",
   "Title": "",
   "Authors": ""
  },
  {
   "Id": "b6",
   "Doi": "10.18653/v1/p16-1223",
   "Title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
   "Authors": "Danqi Chen\nJason Bolton\nChristopher Manning"
  },
  {
   "Id": "b7",
   "Doi": "10.18653/v1/p17-1171",
   "Title": "Reading Wikipedia to Answer Open-Domain Questions",
   "Authors": "Danqi Chen\nAdam Fisch\nJason Weston\nAntoine Bordes"
  },
  {
   "Id": "b8",
   "Doi": "10.18653/v1/p17-1055",
   "Title": "Attention-over-Attention Neural Networks for Reading Comprehension",
   "Authors": "Yiming Cui\nZhipeng Chen\nSi Wei\nShijin Wang\nTing Liu\nGuoping Hu"
  },
  {
   "Id": "b9",
   "Doi": "",
   "Title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
   "Authors": "Jacob Devlin\nMing-Wei Chang\nKenton Lee\nKristina Toutanova"
  },
  {
   "Id": "b10",
   "Doi": "10.18653/v1/p17-1168",
   "Title": "Gated-Attention Readers for Text Comprehension",
   "Authors": "Bhuwan Dhingra\nHanxiao Liu\nZhilin Yang\nWilliam Cohen\nRuslan Salakhutdinov"
  },
  {
   "Id": "b11",
   "Doi": "10.18653/v1/p18-1128",
   "Title": "The Hitchhiker\u2019s Guide to Testing Statistical Significance in Natural Language Processing",
   "Authors": "Rotem Dror\nGili Baumer\nSegev Shlomov\nRoi Reichart"
  },
  {
   "Id": "b12",
   "Doi": "10.1037/e650512012-001",
   "Title": "",
   "Authors": ""
  },
  {
   "Id": "b13",
   "Doi": "",
   "Title": "Teaching Machines to Read and Comprehend",
   "Authors": "Karl Moritz Hermann\nTom\u00e1\u0161 Ko\u010disk\u00fd\nEdward Grefenstette\nLasse Espeholt\nWill Kay\nMustafa Suleyman\nPhil Blunsom"
  },
  {
   "Id": "b14",
   "Doi": "",
   "Title": "",
   "Authors": ""
  },
  {
   "Id": "b15",
   "Doi": "10.18653/v1/p16-1086",
   "Title": "Text Understanding with the Attention Sum Reader Network",
   "Authors": "Rudolf Kadlec\nMartin Schmid\nOnd\u0159ej Bajgar\nJan Kleindienst"
  },
  {
   "Id": "b16",
   "Doi": "10.1162/tacl_a_00276",
   "Title": "Natural Questions: A Benchmark for Question Answering Research",
   "Authors": "Tom Kwiatkowski\nJennimaria Palomaki\nOlivia Redfield\nMichael Collins\nAnkur Parikh\nChris Alberti\nDanielle Epstein\nIllia Polosukhin\nJacob Devlin\nKenton Lee\nKristina Toutanova\nLlion Jones\nMatthew Kelcey\nMing-Wei Chang\nAndrew Dai\nJakob Uszkoreit\nQuoc Le\nSlav Petrov"
  },
  {
   "Id": "b17",
   "Doi": "10.18653/v1/d17-1082",
   "Title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
   "Authors": "Guokun Lai\nQizhe Xie\nHanxiao Liu\nYiming Yang\nEduard Hovy"
  },
  {
   "Id": "b18",
   "Doi": "10.1093/bioinformatics/btt474",
   "Title": "DNorm: disease name normalization with pairwise learning to rank",
   "Authors": "Robert Leaman\nRezarta Islamaj Do\u011fan\nZhiyong Lu"
  },
  {
   "Id": "b19",
   "Doi": "",
   "Title": "The Unified Medical Language System",
   "Authors": "A Donald\nBetsy Lindberg\nAlexa Humphreys\nMccray"
  },
  {
   "Id": "b20",
   "Doi": "10.18653/v1/2020.coling-main.233",
   "Title": "A Vietnamese Dataset for Evaluating Machine Reading Comprehension",
   "Authors": "Kiet Nguyen\nVu Nguyen\nAnh Nguyen\nNgan Nguyen"
  },
  {
   "Id": "b21",
   "Doi": "10.18653/v1/d18-1258",
   "Title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records",
   "Authors": "Anusri Pampari\nPreethi Raghavan\nJennifer Liang\nJian Peng"
  },
  {
   "Id": "b22",
   "Doi": "10.18653/v1/2020.bionlp-1.15",
   "Title": "BioMRC: A Dataset for Biomedical Machine Reading Comprehension",
   "Authors": "Dimitris Pappas\nPetros Stavropoulos\nIon Androutsopoulos\nRyan Mcdonald"
  },
  {
   "Id": "b23",
   "Doi": "10.18653/v1/p18-2124",
   "Title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
   "Authors": "Pranav Rajpurkar\nRobin Jia\nPercy Liang"
  },
  {
   "Id": "b24",
   "Doi": "10.18653/v1/d16-1264",
   "Title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
   "Authors": "Pranav Rajpurkar\nJian Zhang\nKonstantin Lopyrev\nPercy Liang"
  },
  {
   "Id": "b25",
   "Doi": "10.1093/jamia/ocx132",
   "Title": "CLAMP \u2013 a toolkit for efficiently building customized clinical natural language processing pipelines",
   "Authors": "Ergin Soysal\nJingqi Wang\nMin Jiang\nYonghui Wu\nSerguei Pakhomov\nHongfang Liu\nHua Xu"
  },
  {
   "Id": "b26",
   "Doi": "10.18653/v1/n18-1140",
   "Title": "CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension",
   "Authors": "Simon Suster\nWalter Daelemans"
  },
  {
   "Id": "b27",
   "Doi": "10.1177/107769905303000401",
   "Title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability",
   "Authors": "Wilson Taylor"
  },
  {
   "Id": "b28",
   "Doi": "10.18653/v1/w17-2623",
   "Title": "NewsQA: A Machine Comprehension Dataset",
   "Authors": "Adam Trischler\nTong Wang\nXingdi Yuan\nJustin Harris\nAlessandro Sordoni\nPhilip Bachman\nKaheer Suleman"
  },
  {
   "Id": "b29",
   "Doi": "10.1186/s12859-015-0564-6",
   "Title": "An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition",
   "Authors": "George Tsatsaronis\nGeorgios Balikas\nProdromos Malakasiotis\nIoannis Partalas\nMatthias Zschunke\nMichael Alvers\nDirk Weissenborn\nAnastasia Krithara\nSergios Petridis\nDimitris Polychronopoulos\nYannis Almirantis\nJohn Pavlopoulos\nNicolas Baskiotis\nPatrick Gallinari\nThierry Arti\u00e9res\nAxel-Cyrille Ngonga Ngomo\nNorman Heino\nEric Gaussier\nLiliana Barrio-Alvers\nMichael Schroeder\nIon Androutsopoulos\nGeorgios Paliouras"
  },
  {
   "Id": "b30",
   "Doi": "10.1093/database/bas041",
   "Title": "Accelerating literature curation with text-mining tools: a case study of using PubTator to curate genes in PubMed abstracts",
   "Authors": "C-H Wei\nBethany Harris\nDonghui Li\nTanya Berardini\nEva Huala\nH-Y Kao\nZhiyong Lu"
  },
  {
   "Id": "b31",
   "Doi": "10.1609/aaai.v32i1.11970",
   "Title": "Medical Exam Question Answering with Large-scale Reading Comprehension",
   "Authors": "Xiao Zhang\nJi Wu\nZhiyang He\nXien Liu\nYing Su"
  },
  {
   "Id": "b32",
   "Doi": "10.1177/003368828401500213",
   "Title": "Understanding Reading Compre hension",
   "Authors": "Zhuosheng Zhang\nJun Jie Yang\nHai Zhao"
  }
 ],
 "citances": [
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    1,
    2,
    0
   ],
   "CitText": ": \"2004 , 17 , 250 257 .14967013",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "'question' originating from reference : \"2004 , 17 , 250 257 .14967013"
  },
  {
   "Target": "",
   "Type": "figure",
   "Indices": [
    3,
    1,
    0
   ],
   "CitText": "1",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Figure 1: Example passage-question instance of BIOMRC."
  },
  {
   "Target": "",
   "Type": "figure",
   "Indices": [
    3,
    1,
    10
   ],
   "CitText": "1",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "In all versions of BIOMRC (LARGE, LITE, TINY), the entity identifiers of PUBTATOR are replaced by pseudo-identifiers of the form @entityN (Fig. 1), as in the CNN and Daily Mail datasets (Hermann et al., 2015)."
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    3,
    2,
    2
   ],
   "CitText": "[MASK]",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "The top-level embedding produced by SCIBERT for the first sub-token of each candidate answer is concatenated with the toplevel embedding of [MASK] (which replaces the placeholder XXXX) of the question, and they are fed to an MLP, which produces the score of the candidate answer."
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    5,
    1,
    6
   ],
   "CitText": "(Chen et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "These baselines are used to check that the questions cannot be answered by simplistic heuristics (Chen et al., 2016)."
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    5,
    3,
    3
   ],
   "CitText": "[MASK]",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "For each sentence, we concatenate it (using BERT's [SEP] token) with the question, after replacing the XXXX with BERT's [MASK] token, and we feed the concatenation to SCIBERT (Fig. 2)."
  },
  {
   "Target": "",
   "Type": "table",
   "Indices": [
    6,
    0,
    0
   ],
   "CitText": "3",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Table 3 reports the accuracy of all methods on BIOMRC LITE for Settings A and B. In both settings, all the neural models clearly outperform all the basic baselines, with BASE3 (most frequent entity of the passage) performing worst and BASE3+ performing much better, as expected."
  },
  {
   "Target": "",
   "Type": "table",
   "Indices": [
    6,
    2,
    0
   ],
   "CitText": "3",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "In both Settings A and B, AOA-READER performs better than AS-READER, which was expected since it uses a more elaborate attention mechanism, at the expense of taking longer to train (Table 3). 10"
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    6,
    3,
    5
   ],
   "CitText": "41% and 51.19%,",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "41% and 51.19%,respectively (cf. Table 3); in Setting B, it was 50.44% and 49.94%, respectively."
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    6,
    3,
    5
   ],
   "CitText": "respectively (cf. Table 3)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "41% and 51.19%,respectively (cf. Table 3); in Setting B, it was 50.44% and 49.94%, respectively."
  },
  {
   "Target": "",
   "Type": "table",
   "Indices": [
    6,
    3,
    11
   ],
   "CitText": "3",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Overall, however, BASE4 is clearly the best basic baseline, but it is outperformed by all neural models in almost all cases, as in Table 3. SCIBERT-MAX-READER is again the best system in both settings, almost always outperforming the other systems."
  },
  {
   "Target": "",
   "Type": "table",
   "Indices": [
    6,
    3,
    13
   ],
   "CitText": "3",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "AOA-READER is competitive to SCIBERT-SUM-READER in Setting A, and slightly better overall than SCIBERT-SUM-READER in Setting B, as can be seen in Table 3. Pappas et al. (2018) asked humans (non-experts) to answer 30 questions from BIOREAD in Setting A, and 30 other questions in Setting B. We mirrored their experiment by providing 30 questions (from"
  },
  {
   "Target": "",
   "Type": "figure",
   "Indices": [
    8,
    2,
    0
   ],
   "CitText": "4",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Figure 4: Example from BIOMRC TINY."
  },
  {
   "Target": "",
   "Type": "figure",
   "Indices": [
    8,
    3,
    1
   ],
   "CitText": "4",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "As in the experiment of Pappas et al. (2018), in Setting A both the experts and non-experts were also provided with the original names of the biomedical entities (entity names before replacing them with @entityN pseudo-identifiers) to allow them to use prior knowledge; see the top three zones of Fig. 4 for an example."
  },
  {
   "Target": "",
   "Type": "table",
   "Indices": [
    8,
    3,
    10
   ],
   "CitText": "3",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Also, SCIBERT-MAX-READER performs better than both experts and non-experts in Setting A, and better than non-experts in Setting B. However, BIOMRC TINY contains only 30 instances in each setting, and hence the results of Table 4 are less reliable than those from BIOMRC LITE (Table 3)."
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    3
   ],
   "CitText": "\u0160uster et al. used",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "\u0160uster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993)."
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    3
   ],
   "CitText": "Metathesaurus (Lindberg et al., 1993)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "\u0160uster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993)."
  },
  {
   "Target": "",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    0
   ],
   "CitText": "(Chen et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016;Hermann et al., 2015;Dunn et al., 2017;Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016)."
  },
  {
   "Target": "#b0",
   "Type": "bibr",
   "Indices": [
    1,
    1,
    1
   ],
   "CitText": "(Aronson and Lang, 2010)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "They used the full text of unlabeled biomedical articles from PUBMED CENTRAL, 1 and METAMAP (Aronson and Lang, 2010) to annotate the biomedical entities of the articles."
  },
  {
   "Target": "#b1",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    6
   ],
   "CitText": "Bajgar et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Several datasets have been created following this approach either using books (Hill et al., 2016;Bajgar et al., 2016) or news articles (Hermann et al., 2015)."
  },
  {
   "Target": "#b1",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    0
   ],
   "CitText": "Bajgar et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016;Hermann et al., 2015;Dunn et al., 2017;Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016)."
  },
  {
   "Target": "#b2",
   "Type": "bibr",
   "Indices": [
    5,
    3,
    0
   ],
   "CitText": "(Beltagy et al., 2019)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "BERT-based model: We use SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin et al., 2019) model for scientific text."
  },
  {
   "Target": "#b3",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    0
   ],
   "CitText": "(Ben Abacha and Demner-Fushman, 2019)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018;Ben Abacha et al., 2019;Zhang et al., 2018)."
  },
  {
   "Target": "#b3",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    0
   ],
   "CitText": "Ben Abacha et al., 2019;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018;Ben Abacha et al., 2019;Zhang et al., 2018)."
  },
  {
   "Target": "#b5",
   "Type": "bibr",
   "Indices": [
    5,
    3,
    2
   ],
   "CitText": "(Bird et al., 2009)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "For each passage-question instance, we split the passage into sentences using NLTK (Bird et al., 2009)."
  },
  {
   "Target": "#b7",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    5
   ],
   "CitText": "(Chen et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "\u0160uster et al. experimented with the Stanford Reader (Chen et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017), which perform worse than AOA-READER (Cui et al., 2017)."
  },
  {
   "Target": "#b8",
   "Type": "bibr",
   "Indices": [
    1,
    4,
    0
   ],
   "CitText": "(Cui et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We tested on BIOMRC LITE the two deep learning MRC models that Pappas et al. (2018) had tested on BIOREAD LITE, namely Attention Sum Reader (AS-READER) (Kadlec et al., 2016) and Attention Over Attention Reader (AOA-READER) (Cui et al., 2017)."
  },
  {
   "Target": "#b8",
   "Type": "bibr",
   "Indices": [
    5,
    0,
    0
   ],
   "CitText": "(Cui et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We experimented with the four basic baselines (BASE1-4) that Pappas et al. (2018) used in BIOREAD, the two neural MRC models used by the same authors, AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) model we developed."
  },
  {
   "Target": "#b8",
   "Type": "bibr",
   "Indices": [
    5,
    2,
    0
   ],
   "CitText": "(Cui et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Neural baselines: We use the same implementations of AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space."
  },
  {
   "Target": "#b8",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    5
   ],
   "CitText": "(Cui et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "\u0160uster et al. experimented with the Stanford Reader (Chen et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017), which perform worse than AOA-READER (Cui et al., 2017)."
  },
  {
   "Target": "#b9",
   "Type": "bibr",
   "Indices": [
    1,
    4,
    2
   ],
   "CitText": "(Devlin et al., 2019)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We also developed a new BERTbased (Devlin et al., 2019) MRC model, the best version of which (SCIBERT-MAX-READER) performs even better, with its accuracy reaching 80%."
  },
  {
   "Target": "#b9",
   "Type": "bibr",
   "Indices": [
    5,
    0,
    0
   ],
   "CitText": "(Devlin et al., 2019)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We experimented with the four basic baselines (BASE1-4) that Pappas et al. (2018) used in BIOREAD, the two neural MRC models used by the same authors, AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) model we developed."
  },
  {
   "Target": "#b9",
   "Type": "bibr",
   "Indices": [
    5,
    3,
    0
   ],
   "CitText": "(Devlin et al., 2019)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "BERT-based model: We use SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin et al., 2019) model for scientific text."
  },
  {
   "Target": "#b9",
   "Type": "bibr",
   "Indices": [
    5,
    4,
    0
   ],
   "CitText": "(Devlin et al., 2019)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "8 https://www.semanticscholar.org/ 9 BERT's tokenizer splits the entity identifiers into subtokens (Devlin et al., 2019)."
  },
  {
   "Target": "#b10",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    5
   ],
   "CitText": "(Dhingra et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "\u0160uster et al. experimented with the Stanford Reader (Chen et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017), which perform worse than AOA-READER (Cui et al., 2017)."
  },
  {
   "Target": "#b11",
   "Type": "bibr",
   "Indices": [
    5,
    4,
    4
   ],
   "CitText": "(Dror et al., 2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We used singe-tailed Approximate Randomization (Dror et al., 2018), randomly swapping the answers to 50% of the questions for 10k iterations."
  },
  {
   "Target": "#b12",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    0
   ],
   "CitText": "Dunn et al., 2017;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016;Hermann et al., 2015;Dunn et al., 2017;Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016)."
  },
  {
   "Target": "#b13",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    2
   ],
   "CitText": "(Hermann et al., 2015)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "In machine reading comprehension (MRC) (Hermann et al., 2015), a training instance can be automatically constructed by taking an unlabeled passage of multiple sentences, along with another smaller part of text, also unlabeled, usually the next sentence."
  },
  {
   "Target": "#b13",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    6
   ],
   "CitText": "(Hermann et al., 2015)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Several datasets have been created following this approach either using books (Hill et al., 2016;Bajgar et al., 2016) or news articles (Hermann et al., 2015)."
  },
  {
   "Target": "#b13",
   "Type": "bibr",
   "Indices": [
    3,
    1,
    10
   ],
   "CitText": "(Hermann et al., 2015)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "In all versions of BIOMRC (LARGE, LITE, TINY), the entity identifiers of PUBTATOR are replaced by pseudo-identifiers of the form @entityN (Fig. 1), as in the CNN and Daily Mail datasets (Hermann et al., 2015)."
  },
  {
   "Target": "#b13",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    0
   ],
   "CitText": "Hermann et al., 2015;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016;Hermann et al., 2015;Dunn et al., 2017;Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016)."
  },
  {
   "Target": "#b14",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    6
   ],
   "CitText": "(Hill et al., 2016;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Several datasets have been created following this approach either using books (Hill et al., 2016;Bajgar et al., 2016) or news articles (Hermann et al., 2015)."
  },
  {
   "Target": "#b14",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    0
   ],
   "CitText": "(Hill et al., 2016;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016;Hermann et al., 2015;Dunn et al., 2017;Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016)."
  },
  {
   "Target": "#b15",
   "Type": "bibr",
   "Indices": [
    1,
    4,
    0
   ],
   "CitText": "(Kadlec et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We tested on BIOMRC LITE the two deep learning MRC models that Pappas et al. (2018) had tested on BIOREAD LITE, namely Attention Sum Reader (AS-READER) (Kadlec et al., 2016) and Attention Over Attention Reader (AOA-READER) (Cui et al., 2017)."
  },
  {
   "Target": "#b15",
   "Type": "bibr",
   "Indices": [
    5,
    0,
    0
   ],
   "CitText": "(Kadlec et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We experimented with the four basic baselines (BASE1-4) that Pappas et al. (2018) used in BIOREAD, the two neural MRC models used by the same authors, AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) model we developed."
  },
  {
   "Target": "#b15",
   "Type": "bibr",
   "Indices": [
    5,
    2,
    0
   ],
   "CitText": "(Kadlec et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Neural baselines: We use the same implementations of AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space."
  },
  {
   "Target": "#b16",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    1
   ],
   "CitText": "(Kwiatkowski et al., 2019;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)."
  },
  {
   "Target": "#b16",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    2
   ],
   "CitText": "(Kwiatkowski et al., 2019)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "To our knowledge the biggest human annotated corpus is Google's Natural Questions dataset (Kwiatkowski et al., 2019), with approximately 300k human annotated examples."
  },
  {
   "Target": "#b17",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    1
   ],
   "CitText": "Lai et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)."
  },
  {
   "Target": "#b18",
   "Type": "bibr",
   "Indices": [
    1,
    3,
    6
   ],
   "CitText": "(Leaman et al., 2013)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We use DNORM's biomedical entity annotations, which are more accurate than METAMAP's (Leaman et al., 2013)."
  },
  {
   "Target": "#b20",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    7
   ],
   "CitText": "Nguyen et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Datasets of this kind are noisier than MRC datasets containing human-authored questions and manually annotated passage spans that answer them (Rajpurkar et al., 2016(Rajpurkar et al., , 2018;;Nguyen et al., 2016)."
  },
  {
   "Target": "#b20",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    1
   ],
   "CitText": "Nguyen et al., 2016;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)."
  },
  {
   "Target": "#b21",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    0
   ],
   "CitText": "(Pampari et al., 2018;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018;Ben Abacha et al., 2019;Zhang et al., 2018)."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    0,
    0,
    1
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018)."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    1,
    1,
    0
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "To bypass the need for expert annotators and produce a biomedical MRC dataset large enough to train (or pre-train) deep learning models, Pappas et al. (2018) adopted the cloze-style questions approach."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    1,
    3,
    8
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Following Pappas et al. (2018), we release two versions of BIOMRC, LARGE and LITE, containing 812k and 100k instances respectively, for researchers with more or fewer resources, along with the 60 instances (TINY) humans answered."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    1,
    4,
    0
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We tested on BIOMRC LITE the two deep learning MRC models that Pappas et al. (2018) had tested on BIOREAD LITE, namely Attention Sum Reader (AS-READER) (Kadlec et al., 2016) and Attention Over Attention Reader (AOA-READER) (Cui et al., 2017)."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    1,
    4,
    1
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Experimental results show that AS-READER and AOA-READER perform better on BIOMRC, with the accuracy of AOA-READER reaching 70% compared to the corresponding 52% accuracy of Pappas et al. (2018), which is a further indication that the new dataset is less noisy or that at least its task is more feasible."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    4,
    0,
    1
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD dataset."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    5,
    0,
    0
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We experimented with the four basic baselines (BASE1-4) that Pappas et al. (2018) used in BIOREAD, the two neural MRC models used by the same authors, AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) model we developed."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    5,
    2,
    0
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Neural baselines: We use the same implementations of AS-READER (Kadlec et al., 2016) and AOA-READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    6,
    3,
    4
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "AOA-READER was also better than AS-READER in the experiments of Pappas et al. (2018) on a LITE version of their BIOREAD dataset, but the development and test accuracy of AOA-READER in Setting A of BIOREAD was reported to be only 52."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    6,
    3,
    7
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "The results of Pappas et al. (2018) were slightly higher in Setting A than in Setting B, suggesting that AOA-READER was able to benefit from the global scope of entity identifiers, unlike our findings in BIOMRC. 12"
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    6,
    3,
    13
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "AOA-READER is competitive to SCIBERT-SUM-READER in Setting A, and slightly better overall than SCIBERT-SUM-READER in Setting B, as can be seen in Table 3. Pappas et al. (2018) asked humans (non-experts) to answer 30 questions from BIOREAD in Setting A, and 30 other questions in Setting B. We mirrored their experiment by providing 30 questions (from"
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    8,
    3,
    1
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "As in the experiment of Pappas et al. (2018), in Setting A both the experts and non-experts were also provided with the original names of the biomedical entities (entity names before replacing them with @entityN pseudo-identifiers) to allow them to use prior knowledge; see the top three zones of Fig. 4 for an example."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    8,
    4,
    0
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "In the corresponding experiments of Pappas et al. (2018), which were conducted in Setting B only, the average accuracy of the (non-expert) humans was 68.01%, but the humans were also allowed not to answer (when clueless), and unanswered questions were excluded from accuracy."
  },
  {
   "Target": "#b22",
   "Type": "bibr",
   "Indices": [
    10,
    0,
    1
   ],
   "CitText": "Pappas et al. (2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018)."
  },
  {
   "Target": "#b23",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    7
   ],
   "CitText": "(Rajpurkar et al., , 2018;;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Datasets of this kind are noisier than MRC datasets containing human-authored questions and manually annotated passage spans that answer them (Rajpurkar et al., 2016(Rajpurkar et al., , 2018;;Nguyen et al., 2016)."
  },
  {
   "Target": "#b23",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    1
   ],
   "CitText": "Rajpurkar et al., , 2018;;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)."
  },
  {
   "Target": "#b24",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    7
   ],
   "CitText": "(Rajpurkar et al., 2016",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Datasets of this kind are noisier than MRC datasets containing human-authored questions and manually annotated passage spans that answer them (Rajpurkar et al., 2016(Rajpurkar et al., , 2018;;Nguyen et al., 2016)."
  },
  {
   "Target": "#b24",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    9
   ],
   "CitText": "(Rajpurkar et al., 2016)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "For example, the BIOASQ QA dataset (Tsatsaronis et al., 2015) currently contains approximately 3k questions, much fewer than the 100k questions of a SQUAD (Rajpurkar et al., 2016), exactly because it relies on expert annotators."
  },
  {
   "Target": "#b24",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    1
   ],
   "CitText": "Rajpurkar et al., 2016",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)."
  },
  {
   "Target": "#b25",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    3
   ],
   "CitText": "(Soysal et al., 2017)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "\u0160uster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993)."
  },
  {
   "Target": "#b26",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    1
   ],
   "CitText": "( \u0160uster and Daelemans, 2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "The closest dataset to ours is CLICR ( \u0160uster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports. 13"
  },
  {
   "Target": "#b27",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    5
   ],
   "CitText": "(Taylor, 1953)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "This kind of question answering (QA) is also known as cloze-type questions (Taylor, 1953)."
  },
  {
   "Target": "#b28",
   "Type": "bibr",
   "Indices": [
    9,
    2,
    1
   ],
   "CitText": "Trischler et al., 2017;",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019;Rajpurkar et al., 2016Rajpurkar et al., , 2018;;Trischler et al., 2017;Nguyen et al., 2016;Lai et al., 2017)."
  },
  {
   "Target": "#b29",
   "Type": "bibr",
   "Indices": [
    1,
    0,
    9
   ],
   "CitText": "(Tsatsaronis et al., 2015)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "For example, the BIOASQ QA dataset (Tsatsaronis et al., 2015) currently contains approximately 3k questions, much fewer than the 100k questions of a SQUAD (Rajpurkar et al., 2016), exactly because it relies on expert annotators."
  },
  {
   "Target": "#b29",
   "Type": "bibr",
   "Indices": [
    9,
    1,
    0
   ],
   "CitText": "(Tsatsaronis et al., 2015)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "The QA dataset of BIOASQ (Tsatsaronis et al., 2015) contains questions written by biomedical experts."
  },
  {
   "Target": "#b29",
   "Type": "bibr",
   "Indices": [
    10,
    1,
    2
   ],
   "CitText": "(Tsatsaronis et al., 2015)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Finally, we aim to explore if pre-training neural models on BIOREAD is beneficial in human-generated biomedical datasets (Tsatsaronis et al., 2015)."
  },
  {
   "Target": "#b30",
   "Type": "bibr",
   "Indices": [
    1,
    3,
    5
   ],
   "CitText": "(Wei et al., 2012)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Unlike BIOREAD, we use PUBTATOR (Wei et al., 2012), a repository that provides approximately 25 million abstracts and their corresponding titles from PUBMED, with multiple annotations. 2"
  },
  {
   "Target": "#b31",
   "Type": "bibr",
   "Indices": [
    9,
    0,
    0
   ],
   "CitText": "Zhang et al., 2018)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018;Ben Abacha et al., 2019;Zhang et al., 2018)."
  },
  {
   "Target": "#b32",
   "Type": "bibr",
   "Indices": [
    10,
    1,
    1
   ],
   "CitText": "(Zhang et al., 2020)",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We also plan to experiment with other MRC models that recently performed particularly well on opendomain MRC datasets (Zhang et al., 2020)."
  },
  {
   "Target": "#fig_0",
   "Type": "figure",
   "Indices": [
    5,
    3,
    3
   ],
   "CitText": "2",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "For each sentence, we concatenate it (using BERT's [SEP] token) with the question, after replacing the XXXX with BERT's [MASK] token, and we feed the concatenation to SCIBERT (Fig. 2)."
  },
  {
   "Target": "#fig_1",
   "Type": "figure",
   "Indices": [
    6,
    3,
    8
   ],
   "CitText": "3",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "igure 3 shows how many passage-question instances of the development subset of BIOMRC LITE have 2, 3, . . ."
  },
  {
   "Target": "#foot_0",
   "Type": "foot",
   "Indices": [
    1,
    3,
    5
   ],
   "CitText": "2",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Unlike BIOREAD, we use PUBTATOR (Wei et al., 2012), a repository that provides approximately 25 million abstracts and their corresponding titles from PUBMED, with multiple annotations. 2"
  },
  {
   "Target": "#foot_1",
   "Type": "foot",
   "Indices": [
    1,
    4,
    3
   ],
   "CitText": "3",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We encourage further research on biomedical MRC by making our code and data publicly available, and by creating an on-line leaderboard for BIOMRC.3"
  },
  {
   "Target": "#foot_2",
   "Type": "foot",
   "Indices": [
    2,
    0,
    5
   ],
   "CitText": "4",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We also discarded articles containing entities not linked to any of the ontologies used by PUBTATOR,4 or entities linked to multiple ontologies (entities with multiple ids), or entities whose spans overlapped with those of other entities."
  },
  {
   "Target": "#foot_3",
   "Type": "foot",
   "Indices": [
    2,
    0,
    6
   ],
   "CitText": "5",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "We also removed articles with no entities in their titles, and articles with no entities shared by the title and abstract. 5"
  },
  {
   "Target": "#foot_4",
   "Type": "foot",
   "Indices": [
    6,
    2,
    0
   ],
   "CitText": "10",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "In both Settings A and B, AOA-READER performs better than AS-READER, which was expected since it uses a more elaborate attention mechanism, at the expense of taking longer to train (Table 3). 10"
  },
  {
   "Target": "#foot_5",
   "Type": "foot",
   "Indices": [
    6,
    3,
    0
   ],
   "CitText": "11",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "The trainable parameters of AS-READER and AOA-READER are almost double in Setting A compared to Setting B. To some extent, this difference is due to the fact that for both models we learn a word embedding for each @entityN pseudoidentifier, and in Setting A the numbering of the identifiers is not reset for each passage-question instance, leading to many more pseudo-identifiers (31.77k pseudo-identifiers in the vocabulary of Setting A vs. only 20 in Setting B); this accounts for a difference of 1.59M parameters. 11"
  },
  {
   "Target": "#foot_6",
   "Type": "foot",
   "Indices": [
    6,
    3,
    7
   ],
   "CitText": "12",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "The results of Pappas et al. (2018) were slightly higher in Setting A than in Setting B, suggesting that AOA-READER was able to benefit from the global scope of entity identifiers, unlike our findings in BIOMRC. 12"
  },
  {
   "Target": "#tab_1",
   "Type": "table",
   "Indices": [
    1,
    2,
    2
   ],
   "CitText": "1",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Many instances contain passages or questions crossing article sections, or originating from the references sections of articles, or they include captions and footnotes (Table 1)."
  },
  {
   "Target": "#tab_2",
   "Type": "table",
   "Indices": [
    3,
    1,
    8
   ],
   "CitText": "2",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "812k passage-question instances, which form BIOMRC LARGE, split into training, development, and test subsets (Table 2)."
  },
  {
   "Target": "#tab_2",
   "Type": "table",
   "Indices": [
    3,
    2,
    0
   ],
   "CitText": "2",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Table 2 provides statistics on BIOMRC."
  },
  {
   "Target": "#tab_4",
   "Type": "table",
   "Indices": [
    8,
    3,
    3
   ],
   "CitText": "4",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Table 4 reports the human and system accuracy scores on BIOMRC TINY."
  },
  {
   "Target": "#tab_4",
   "Type": "table",
   "Indices": [
    8,
    3,
    10
   ],
   "CitText": "4",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Also, SCIBERT-MAX-READER performs better than both experts and non-experts in Setting A, and better than non-experts in Setting B. However, BIOMRC TINY contains only 30 instances in each setting, and hence the results of Table 4 are less reliable than those from BIOMRC LITE (Table 3)."
  },
  {
   "Target": "#tab_5",
   "Type": "table",
   "Indices": [
    8,
    5,
    0
   ],
   "CitText": "5",
   "Intent": "Skipped",
   "Polarity": "Skipped",
   "Semantics": "Skipped",
   "Intent Score": "Skipped",
   "Polarity Score": "Skipped",
   "Semantics Score": "Skipped",
   "Sentence": "Inter-annotator agreement was also higher for experts than non-experts in our experiment, in both Settings A and B (Table 5)."
  }
 ]
}